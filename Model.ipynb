{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1662997f-35f2-483a-93fd-749d8c6cee13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from types import GeneratorType\n",
    "#Nested cross validation\n",
    "from cv import NestedCV\n",
    "from darts import TimeSeries\n",
    "from darts.models import RNNModel,NBEATSModel,XGBModel\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.metrics import mape\n",
    "from prophet import Prophet\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93a09d3-df27-4017-8f27-3d23dbab3e82",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "<h3>\n",
    "1)Test cases to check the implementation of NestedCV <br/>\n",
    "2)Build a timeseries model and use the cross validation technique for model evaluation\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73542d2d-53c8-4933-8d98-e5906eb81bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating sample dataset for testing NestedCV\n",
    "start_date = datetime(2022, 1, 1)\n",
    "end_date = datetime(2022, 3, 31)\n",
    "date_range = pd.date_range(start_date, end_date, freq='D')\n",
    "\n",
    "data = {\n",
    "    'date': np.repeat(date_range, 3),  # Repeat each date three times for three groups\n",
    "    'group_column': np.tile(['Group1', 'Group2', 'Group3'], len(date_range)),\n",
    "    'value': np.random.randn(len(date_range) * 3)\n",
    "}\n",
    "data = pd.DataFrame(data)\n",
    "data[\"date\"] = pd.to_datetime(data[\"date\"])\n",
    "\n",
    "# nested cv\n",
    "k = 5\n",
    "cv = NestedCV(k)\n",
    "splits = cv.split(data, \"date\")\n",
    "\n",
    "# check return type\n",
    "assert isinstance(splits, GeneratorType)\n",
    "\n",
    "# check return types, shapes, and data leaks\n",
    "count = 0\n",
    "for train, validate in splits:\n",
    "    \n",
    "    # types\n",
    "    assert isinstance(train, pd.DataFrame)\n",
    "    assert isinstance(validate, pd.DataFrame)\n",
    "\n",
    "    # shape\n",
    "    assert train.shape[1] == validate.shape[1]\n",
    "\n",
    "    # data leak\n",
    "    assert train[\"date\"].max() <= validate[\"date\"].min()\n",
    "\n",
    "    count += 1\n",
    "\n",
    "# check number of splits returned\n",
    "assert count == k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a3cf2bf-d46e-4904-ac4b-49d71865064f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Fold 1-----------\n",
      "Train\n",
      "         date group_column     value\n",
      "0  2022-01-01       Group1 -0.446453\n",
      "1  2022-01-01       Group2 -0.589737\n",
      "2  2022-01-01       Group3 -0.857503\n",
      "3  2022-01-02       Group1  0.382831\n",
      "4  2022-01-02       Group2 -0.978178\n",
      "5  2022-01-02       Group3 -0.611634\n",
      "6  2022-01-03       Group1 -0.363782\n",
      "7  2022-01-03       Group2 -0.351690\n",
      "8  2022-01-03       Group3  1.167147\n",
      "9  2022-01-04       Group1  2.218697\n",
      "10 2022-01-04       Group2  0.469655\n",
      "11 2022-01-04       Group3 -0.646692\n",
      "12 2022-01-05       Group1 -0.499362\n",
      "13 2022-01-05       Group2 -0.309289\n",
      "14 2022-01-05       Group3 -0.099819\n",
      "15 2022-01-06       Group1  0.317343\n",
      "16 2022-01-06       Group2 -0.821913\n",
      "17 2022-01-06       Group3 -0.027728\n",
      "18 2022-01-07       Group1  0.087734\n",
      "19 2022-01-07       Group2  0.664127\n",
      "20 2022-01-07       Group3  0.175308\n",
      "21 2022-01-08       Group1 -0.289988\n",
      "22 2022-01-08       Group2 -0.107586\n",
      "23 2022-01-08       Group3  0.406700\n",
      "24 2022-01-09       Group1 -0.645324\n",
      "25 2022-01-09       Group2 -0.114373\n",
      "26 2022-01-09       Group3  0.997187\n",
      "27 2022-01-10       Group1  1.328417\n",
      "28 2022-01-10       Group2 -0.083940\n",
      "29 2022-01-10       Group3 -0.859918\n",
      "30 2022-01-11       Group1  0.219145\n",
      "31 2022-01-11       Group2  0.554126\n",
      "32 2022-01-11       Group3 -0.398700\n",
      "33 2022-01-12       Group1  0.926615\n",
      "34 2022-01-12       Group2  1.530668\n",
      "35 2022-01-12       Group3  0.352644\n",
      "36 2022-01-13       Group1 -0.919491\n",
      "37 2022-01-13       Group2  0.168586\n",
      "38 2022-01-13       Group3  0.724720\n",
      "39 2022-01-14       Group1  0.491870\n",
      "40 2022-01-14       Group2 -0.330436\n",
      "41 2022-01-14       Group3  1.933642\n",
      "42 2022-01-15       Group1  1.288237\n",
      "43 2022-01-15       Group2 -0.745238\n",
      "44 2022-01-15       Group3 -2.021603\n",
      "Validate\n",
      "         date group_column     value\n",
      "45 2022-01-16       Group1 -0.246781\n",
      "46 2022-01-16       Group2 -0.069972\n",
      "47 2022-01-16       Group3 -2.131481\n",
      "48 2022-01-17       Group1  0.229800\n",
      "49 2022-01-17       Group2 -1.850326\n",
      "50 2022-01-17       Group3  1.625826\n",
      "51 2022-01-18       Group1  0.381972\n",
      "52 2022-01-18       Group2 -1.194524\n",
      "53 2022-01-18       Group3 -0.416812\n",
      "54 2022-01-19       Group1  1.213011\n",
      "55 2022-01-19       Group2  0.605130\n",
      "56 2022-01-19       Group3  0.141220\n",
      "57 2022-01-20       Group1 -3.224102\n",
      "58 2022-01-20       Group2  0.809793\n",
      "59 2022-01-20       Group3 -0.394647\n",
      "60 2022-01-21       Group1  0.009804\n",
      "61 2022-01-21       Group2 -0.498026\n",
      "62 2022-01-21       Group3 -0.448680\n",
      "63 2022-01-22       Group1 -0.948210\n",
      "64 2022-01-22       Group2 -2.025762\n",
      "65 2022-01-22       Group3 -0.022154\n",
      "66 2022-01-23       Group1  1.772094\n",
      "67 2022-01-23       Group2 -1.404243\n",
      "68 2022-01-23       Group3 -0.341338\n",
      "69 2022-01-24       Group1 -1.174367\n",
      "70 2022-01-24       Group2 -0.642334\n",
      "71 2022-01-24       Group3  1.273570\n",
      "72 2022-01-25       Group1 -1.962452\n",
      "73 2022-01-25       Group2 -0.610429\n",
      "74 2022-01-25       Group3  0.289856\n",
      "75 2022-01-26       Group1  0.167437\n",
      "76 2022-01-26       Group2  0.563432\n",
      "77 2022-01-26       Group3 -0.168168\n",
      "78 2022-01-27       Group1  0.096941\n",
      "79 2022-01-27       Group2  0.479856\n",
      "80 2022-01-27       Group3  0.404310\n",
      "81 2022-01-28       Group1 -0.861633\n",
      "82 2022-01-28       Group2  0.052791\n",
      "83 2022-01-28       Group3  0.506727\n",
      "84 2022-01-29       Group1 -0.748134\n",
      "85 2022-01-29       Group2  0.727143\n",
      "86 2022-01-29       Group3 -1.499739\n",
      "87 2022-01-30       Group1 -0.092817\n",
      "88 2022-01-30       Group2 -0.443262\n",
      "89 2022-01-30       Group3 -1.118557\n",
      "---------Fold 2-----------\n",
      "Train\n",
      "         date group_column     value\n",
      "0  2022-01-01       Group1 -0.446453\n",
      "1  2022-01-01       Group2 -0.589737\n",
      "2  2022-01-01       Group3 -0.857503\n",
      "3  2022-01-02       Group1  0.382831\n",
      "4  2022-01-02       Group2 -0.978178\n",
      "..        ...          ...       ...\n",
      "85 2022-01-29       Group2  0.727143\n",
      "86 2022-01-29       Group3 -1.499739\n",
      "87 2022-01-30       Group1 -0.092817\n",
      "88 2022-01-30       Group2 -0.443262\n",
      "89 2022-01-30       Group3 -1.118557\n",
      "\n",
      "[90 rows x 3 columns]\n",
      "Validate\n",
      "          date group_column     value\n",
      "90  2022-01-31       Group1  0.352672\n",
      "91  2022-01-31       Group2 -0.193331\n",
      "92  2022-01-31       Group3 -2.945644\n",
      "93  2022-02-01       Group1  1.929425\n",
      "94  2022-02-01       Group2  1.195436\n",
      "95  2022-02-01       Group3  0.524220\n",
      "96  2022-02-02       Group1  0.469285\n",
      "97  2022-02-02       Group2  0.081958\n",
      "98  2022-02-02       Group3  1.615706\n",
      "99  2022-02-03       Group1 -0.731962\n",
      "100 2022-02-03       Group2  0.702508\n",
      "101 2022-02-03       Group3  0.886861\n",
      "102 2022-02-04       Group1  0.606318\n",
      "103 2022-02-04       Group2  1.394230\n",
      "104 2022-02-04       Group3  0.398571\n",
      "105 2022-02-05       Group1 -1.930456\n",
      "106 2022-02-05       Group2  0.074549\n",
      "107 2022-02-05       Group3  0.633867\n",
      "108 2022-02-06       Group1  0.238201\n",
      "109 2022-02-06       Group2 -1.419137\n",
      "110 2022-02-06       Group3  0.111982\n",
      "111 2022-02-07       Group1 -0.035068\n",
      "112 2022-02-07       Group2  0.092634\n",
      "113 2022-02-07       Group3  0.345124\n",
      "114 2022-02-08       Group1 -0.698864\n",
      "115 2022-02-08       Group2 -1.096963\n",
      "116 2022-02-08       Group3  0.073424\n",
      "117 2022-02-09       Group1  0.749547\n",
      "118 2022-02-09       Group2 -2.858397\n",
      "119 2022-02-09       Group3  0.504809\n",
      "120 2022-02-10       Group1  1.282246\n",
      "121 2022-02-10       Group2 -0.375361\n",
      "122 2022-02-10       Group3 -0.314758\n",
      "123 2022-02-11       Group1 -1.127067\n",
      "124 2022-02-11       Group2 -0.808978\n",
      "125 2022-02-11       Group3  0.211053\n",
      "126 2022-02-12       Group1 -0.272160\n",
      "127 2022-02-12       Group2  1.101659\n",
      "128 2022-02-12       Group3 -0.581612\n",
      "129 2022-02-13       Group1 -0.478287\n",
      "130 2022-02-13       Group2 -1.427845\n",
      "131 2022-02-13       Group3  0.629917\n",
      "132 2022-02-14       Group1  0.040977\n",
      "133 2022-02-14       Group2  0.786986\n",
      "134 2022-02-14       Group3  0.200004\n",
      "---------Fold 3-----------\n",
      "Train\n",
      "          date group_column     value\n",
      "0   2022-01-01       Group1 -0.446453\n",
      "1   2022-01-01       Group2 -0.589737\n",
      "2   2022-01-01       Group3 -0.857503\n",
      "3   2022-01-02       Group1  0.382831\n",
      "4   2022-01-02       Group2 -0.978178\n",
      "..         ...          ...       ...\n",
      "130 2022-02-13       Group2 -1.427845\n",
      "131 2022-02-13       Group3  0.629917\n",
      "132 2022-02-14       Group1  0.040977\n",
      "133 2022-02-14       Group2  0.786986\n",
      "134 2022-02-14       Group3  0.200004\n",
      "\n",
      "[135 rows x 3 columns]\n",
      "Validate\n",
      "          date group_column     value\n",
      "135 2022-02-15       Group1 -0.943342\n",
      "136 2022-02-15       Group2 -0.534149\n",
      "137 2022-02-15       Group3  0.614986\n",
      "138 2022-02-16       Group1  0.140479\n",
      "139 2022-02-16       Group2 -1.413144\n",
      "140 2022-02-16       Group3 -0.770921\n",
      "141 2022-02-17       Group1  0.553387\n",
      "142 2022-02-17       Group2 -0.617485\n",
      "143 2022-02-17       Group3  1.279602\n",
      "144 2022-02-18       Group1 -0.914383\n",
      "145 2022-02-18       Group2  0.622715\n",
      "146 2022-02-18       Group3  0.816237\n",
      "147 2022-02-19       Group1  0.413803\n",
      "148 2022-02-19       Group2 -0.690494\n",
      "149 2022-02-19       Group3  2.246668\n",
      "150 2022-02-20       Group1  0.877567\n",
      "151 2022-02-20       Group2 -0.028164\n",
      "152 2022-02-20       Group3  0.358885\n",
      "153 2022-02-21       Group1  0.477884\n",
      "154 2022-02-21       Group2  0.901953\n",
      "155 2022-02-21       Group3 -1.859173\n",
      "156 2022-02-22       Group1  0.787498\n",
      "157 2022-02-22       Group2 -0.204548\n",
      "158 2022-02-22       Group3 -0.356481\n",
      "159 2022-02-23       Group1 -0.711441\n",
      "160 2022-02-23       Group2 -1.491408\n",
      "161 2022-02-23       Group3 -0.540303\n",
      "162 2022-02-24       Group1 -0.057583\n",
      "163 2022-02-24       Group2 -0.467317\n",
      "164 2022-02-24       Group3  0.707126\n",
      "165 2022-02-25       Group1  0.391410\n",
      "166 2022-02-25       Group2 -2.117816\n",
      "167 2022-02-25       Group3  2.808396\n",
      "168 2022-02-26       Group1  1.014281\n",
      "169 2022-02-26       Group2  1.240482\n",
      "170 2022-02-26       Group3 -0.022412\n",
      "171 2022-02-27       Group1 -0.097958\n",
      "172 2022-02-27       Group2 -0.113445\n",
      "173 2022-02-27       Group3  0.816125\n",
      "174 2022-02-28       Group1  0.436230\n",
      "175 2022-02-28       Group2  0.552230\n",
      "176 2022-02-28       Group3 -0.552377\n",
      "177 2022-03-01       Group1  1.068181\n",
      "178 2022-03-01       Group2  1.014431\n",
      "179 2022-03-01       Group3  0.446459\n",
      "---------Fold 4-----------\n",
      "Train\n",
      "          date group_column     value\n",
      "0   2022-01-01       Group1 -0.446453\n",
      "1   2022-01-01       Group2 -0.589737\n",
      "2   2022-01-01       Group3 -0.857503\n",
      "3   2022-01-02       Group1  0.382831\n",
      "4   2022-01-02       Group2 -0.978178\n",
      "..         ...          ...       ...\n",
      "175 2022-02-28       Group2  0.552230\n",
      "176 2022-02-28       Group3 -0.552377\n",
      "177 2022-03-01       Group1  1.068181\n",
      "178 2022-03-01       Group2  1.014431\n",
      "179 2022-03-01       Group3  0.446459\n",
      "\n",
      "[180 rows x 3 columns]\n",
      "Validate\n",
      "          date group_column     value\n",
      "180 2022-03-02       Group1  0.965215\n",
      "181 2022-03-02       Group2 -0.696658\n",
      "182 2022-03-02       Group3 -2.034085\n",
      "183 2022-03-03       Group1  0.086736\n",
      "184 2022-03-03       Group2  2.549015\n",
      "185 2022-03-03       Group3 -1.427208\n",
      "186 2022-03-04       Group1 -0.145005\n",
      "187 2022-03-04       Group2 -0.618264\n",
      "188 2022-03-04       Group3  0.315851\n",
      "189 2022-03-05       Group1  0.315988\n",
      "190 2022-03-05       Group2 -0.698208\n",
      "191 2022-03-05       Group3  0.321422\n",
      "192 2022-03-06       Group1 -0.832482\n",
      "193 2022-03-06       Group2  0.319641\n",
      "194 2022-03-06       Group3  1.947984\n",
      "195 2022-03-07       Group1  0.024412\n",
      "196 2022-03-07       Group2 -0.072901\n",
      "197 2022-03-07       Group3  1.168049\n",
      "198 2022-03-08       Group1  0.935469\n",
      "199 2022-03-08       Group2  1.273484\n",
      "200 2022-03-08       Group3  1.543346\n",
      "201 2022-03-09       Group1 -1.251477\n",
      "202 2022-03-09       Group2  1.538818\n",
      "203 2022-03-09       Group3  0.540024\n",
      "204 2022-03-10       Group1  0.178331\n",
      "205 2022-03-10       Group2 -0.111074\n",
      "206 2022-03-10       Group3 -0.998876\n",
      "207 2022-03-11       Group1 -0.911594\n",
      "208 2022-03-11       Group2  0.689808\n",
      "209 2022-03-11       Group3  0.458378\n",
      "210 2022-03-12       Group1  0.190091\n",
      "211 2022-03-12       Group2  1.553068\n",
      "212 2022-03-12       Group3 -0.550894\n",
      "213 2022-03-13       Group1  0.438325\n",
      "214 2022-03-13       Group2  1.229405\n",
      "215 2022-03-13       Group3 -0.279929\n",
      "216 2022-03-14       Group1 -0.573118\n",
      "217 2022-03-14       Group2  0.899069\n",
      "218 2022-03-14       Group3 -1.925962\n",
      "219 2022-03-15       Group1 -0.037998\n",
      "220 2022-03-15       Group2 -0.056582\n",
      "221 2022-03-15       Group3 -1.002482\n",
      "222 2022-03-16       Group1  0.826875\n",
      "223 2022-03-16       Group2 -0.937395\n",
      "224 2022-03-16       Group3 -1.244710\n",
      "---------Fold 5-----------\n",
      "Train\n",
      "          date group_column     value\n",
      "0   2022-01-01       Group1 -0.446453\n",
      "1   2022-01-01       Group2 -0.589737\n",
      "2   2022-01-01       Group3 -0.857503\n",
      "3   2022-01-02       Group1  0.382831\n",
      "4   2022-01-02       Group2 -0.978178\n",
      "..         ...          ...       ...\n",
      "220 2022-03-15       Group2 -0.056582\n",
      "221 2022-03-15       Group3 -1.002482\n",
      "222 2022-03-16       Group1  0.826875\n",
      "223 2022-03-16       Group2 -0.937395\n",
      "224 2022-03-16       Group3 -1.244710\n",
      "\n",
      "[225 rows x 3 columns]\n",
      "Validate\n",
      "          date group_column     value\n",
      "225 2022-03-17       Group1  1.565178\n",
      "226 2022-03-17       Group2 -0.302583\n",
      "227 2022-03-17       Group3 -0.328883\n",
      "228 2022-03-18       Group1  0.372193\n",
      "229 2022-03-18       Group2  0.008324\n",
      "230 2022-03-18       Group3  0.243331\n",
      "231 2022-03-19       Group1 -0.691060\n",
      "232 2022-03-19       Group2 -0.153111\n",
      "233 2022-03-19       Group3 -0.138625\n",
      "234 2022-03-20       Group1 -0.887788\n",
      "235 2022-03-20       Group2  0.274228\n",
      "236 2022-03-20       Group3 -1.545585\n",
      "237 2022-03-21       Group1  0.850728\n",
      "238 2022-03-21       Group2  1.273642\n",
      "239 2022-03-21       Group3  0.221090\n",
      "240 2022-03-22       Group1 -0.312064\n",
      "241 2022-03-22       Group2  1.550647\n",
      "242 2022-03-22       Group3  1.779705\n",
      "243 2022-03-23       Group1  2.082042\n",
      "244 2022-03-23       Group2  1.158448\n",
      "245 2022-03-23       Group3 -0.611129\n",
      "246 2022-03-24       Group1 -0.027010\n",
      "247 2022-03-24       Group2 -0.079625\n",
      "248 2022-03-24       Group3  0.453928\n",
      "249 2022-03-25       Group1  0.933587\n",
      "250 2022-03-25       Group2  0.528586\n",
      "251 2022-03-25       Group3  1.070325\n",
      "252 2022-03-26       Group1  0.063078\n",
      "253 2022-03-26       Group2 -0.597930\n",
      "254 2022-03-26       Group3  0.876839\n",
      "255 2022-03-27       Group1 -0.549514\n",
      "256 2022-03-27       Group2  0.535475\n",
      "257 2022-03-27       Group3 -0.407426\n",
      "258 2022-03-28       Group1 -1.185621\n",
      "259 2022-03-28       Group2 -0.311541\n",
      "260 2022-03-28       Group3 -0.678218\n",
      "261 2022-03-29       Group1  0.672182\n",
      "262 2022-03-29       Group2  0.710329\n",
      "263 2022-03-29       Group3  2.201106\n",
      "264 2022-03-30       Group1  0.613753\n",
      "265 2022-03-30       Group2 -0.364689\n",
      "266 2022-03-30       Group3 -0.357747\n",
      "267 2022-03-31       Group1 -0.468571\n",
      "268 2022-03-31       Group2  0.982470\n",
      "269 2022-03-31       Group3 -0.468444\n"
     ]
    }
   ],
   "source": [
    "#Visualizing the train and validation sets of above dataset for each fold\n",
    "fold=1\n",
    "cv = NestedCV(k)\n",
    "splits = cv.split(data, \"date\")\n",
    "for train, validate in splits:\n",
    "    print(f\"---------Fold {fold}-----------\")\n",
    "    print(\"Train\")\n",
    "    print(train)\n",
    "    print(\"Validate\")\n",
    "    print(validate)\n",
    "    fold=fold+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7806234-37e1-4586-bb79-7401afdddcc4",
   "metadata": {},
   "source": [
    "# We will be using the dataset provided in the assessment to construct basic time series models and validate our cross-validation logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c89a329-a425-46d8-87ed-066074e6bfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train.csv\")\n",
    "data=data[~data[\"date\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ddcf651-7a0c-4cef-ade8-47c8aa7aeea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'M'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this dataset contains monthly frequency.\n",
    "pd.infer_freq(data['date'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee924e3a-c53c-466e-a08e-cd9f350bbf0c",
   "metadata": {},
   "source": [
    "<h2>We will be building a time series model to forecast demand for each brand</h2>\n",
    "\n",
    "1)First, we will try simple machine learning models such as Facebook Prophet.  <br/>\n",
    "2)Next, we will explore some deep learning models. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7f4fc7c-a5a2-45e3-b826-2dba05a98088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['kinder-cola', 'adult-cola', 'orange-power', 'gazoza',\n",
       "       'lemon-boost'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of unique Brands\n",
    "data[\"brand\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b893cd5b-2679-44cb-8078-2749ecb7802a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kinder-cola\n",
      "Data points 1296\n",
      "Start Date: 28/02/13\n",
      "End Date: 31/12/17\n",
      "adult-cola\n",
      "Data points 1296\n",
      "Start Date: 28/02/13\n",
      "End Date: 31/12/17\n",
      "orange-power\n",
      "Data points 1296\n",
      "Start Date: 28/02/13\n",
      "End Date: 31/12/17\n",
      "gazoza\n",
      "Data points 1296\n",
      "Start Date: 28/02/13\n",
      "End Date: 31/12/17\n",
      "lemon-boost\n",
      "Data points 1296\n",
      "Start Date: 28/02/13\n",
      "End Date: 31/12/17\n"
     ]
    }
   ],
   "source": [
    "# Check the number of data points, maximum date, and minimum date for each brand\n",
    "for b in data[\"brand\"].unique():\n",
    "    print(b)\n",
    "    temp=data[data[\"brand\"]==b]\n",
    "    print(f\"Data points {len(temp)}\")\n",
    "    print(f\"Start Date: {temp['date'].min()}\")\n",
    "    print(f\"End Date: {temp['date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22c0f0ca-b800-4b8f-93c7-047ac853db2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are no missing values in the dataset\n",
    "data=data[[\"date\",\"brand\",\"quantity\"]]\n",
    "data['date'] = pd.to_datetime(data['date'], format='%d/%m/%y')\n",
    "data.set_index('date', inplace=True)\n",
    "#Resample the data to monthly frequency on the basis of brand\n",
    "resampled_data=data.groupby(['brand', pd.Grouper(freq='M')]).sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75f7745a-511e-4470-82d4-e897cb798d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand</th>\n",
       "      <th>date</th>\n",
       "      <th>quantity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adult-cola</td>\n",
       "      <td>2012-01-31</td>\n",
       "      <td>315852.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adult-cola</td>\n",
       "      <td>2012-02-29</td>\n",
       "      <td>232201.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adult-cola</td>\n",
       "      <td>2012-03-31</td>\n",
       "      <td>359081.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adult-cola</td>\n",
       "      <td>2012-04-30</td>\n",
       "      <td>444008.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>adult-cola</td>\n",
       "      <td>2012-05-31</td>\n",
       "      <td>469280.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>orange-power</td>\n",
       "      <td>2017-08-31</td>\n",
       "      <td>776399.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>orange-power</td>\n",
       "      <td>2017-09-30</td>\n",
       "      <td>646508.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>orange-power</td>\n",
       "      <td>2017-10-31</td>\n",
       "      <td>550004.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>orange-power</td>\n",
       "      <td>2017-11-30</td>\n",
       "      <td>434359.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>orange-power</td>\n",
       "      <td>2017-12-31</td>\n",
       "      <td>650151.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>360 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            brand       date  quantity\n",
       "0      adult-cola 2012-01-31  315852.0\n",
       "1      adult-cola 2012-02-29  232201.0\n",
       "2      adult-cola 2012-03-31  359081.0\n",
       "3      adult-cola 2012-04-30  444008.0\n",
       "4      adult-cola 2012-05-31  469280.0\n",
       "..            ...        ...       ...\n",
       "355  orange-power 2017-08-31  776399.0\n",
       "356  orange-power 2017-09-30  646508.0\n",
       "357  orange-power 2017-10-31  550004.0\n",
       "358  orange-power 2017-11-30  434359.0\n",
       "359  orange-power 2017-12-31  650151.0\n",
       "\n",
       "[360 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resampled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efada48-1dbf-402a-8259-83e098b9bdee",
   "metadata": {},
   "source": [
    "## Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7658f037-a08b-4451-aeb9-b14aaa18eddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Fold 1--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:40:00 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:40:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:40:00 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:40:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:40:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:40:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:40:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:40:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:40:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:40:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:40:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:40:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:40:01 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Fold 2--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:40:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:40:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:40:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:40:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:40:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:40:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:40:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:40:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:40:02 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Fold 3--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:40:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:40:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:40:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:40:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:40:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:40:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:40:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:40:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:40:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:40:03 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Fold 4--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:40:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:40:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:40:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:40:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:40:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:40:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:40:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:40:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:40:04 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Fold 5--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:40:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:40:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:40:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:40:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:40:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:40:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:40:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:40:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:40:05 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    }
   ],
   "source": [
    "# Initialize Nested Cross-Validation with 5 folds\n",
    "cv = NestedCV(5)\n",
    "splits = cv.split(resampled_data, \"date\")\n",
    "fold=1\n",
    "final_result=pd.DataFrame({\"Brand\":[],\"MAPE\":[],\"Fold\":[]})\n",
    "\n",
    "# Loop through each train-validation split in the cross-validation\n",
    "for train, validate in splits:\n",
    "    print(f\"------ Fold {fold}--------\")\n",
    "    for brand, group in train.groupby('brand'): \n",
    "        temp=group[[\"date\",\"quantity\"]]\n",
    "        temp.columns=[\"ds\",\"y\"]\n",
    "        # Initialize and fit the Prophet model\n",
    "        m=Prophet()\n",
    "        m.fit(temp)\n",
    "        # Prepare validation data for forecasting\n",
    "        v_temp=validate[validate[\"brand\"]==brand][[\"date\",\"quantity\"]]\n",
    "        v_temp.columns=[\"ds\",\"y\"]\n",
    "        # Generate forecast using the trained Prophet model\n",
    "        forecast = m.predict(v_temp[[\"ds\"]])\n",
    "        # Merge actual and forecasted data\n",
    "        result=pd.merge(v_temp, forecast[[\"ds\",\"yhat\"]], on='ds')\n",
    "        # Calculate Absolute Percentage Error (APE) and Mean Absolute Percentage Error (MAPE)\n",
    "        result['APE'] = np.abs((result['y'] - result['yhat']) / result['y']) * 100\n",
    "        mape = np.mean(result['APE'])\n",
    "        # print(f'MAPE of {brand}: {mape:.2f}%')\n",
    "        final_result = pd.concat([final_result, pd.DataFrame([{\"Brand\": brand, \"MAPE\": mape, \"Fold\": fold}])], ignore_index=True)\n",
    "\n",
    "    fold=fold+1 \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8aa226f8-4965-4162-acdc-9f5e2cff707f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>Fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>orange-power</td>\n",
       "      <td>5.804694</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>lemon-boost</td>\n",
       "      <td>6.218594</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>kinder-cola</td>\n",
       "      <td>5.252834</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>gazoza</td>\n",
       "      <td>10.742014</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>adult-cola</td>\n",
       "      <td>5.304644</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>orange-power</td>\n",
       "      <td>7.972170</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>lemon-boost</td>\n",
       "      <td>6.218352</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>kinder-cola</td>\n",
       "      <td>5.481717</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>gazoza</td>\n",
       "      <td>13.349993</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>adult-cola</td>\n",
       "      <td>11.457783</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>lemon-boost</td>\n",
       "      <td>11.343404</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>orange-power</td>\n",
       "      <td>10.647084</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>kinder-cola</td>\n",
       "      <td>9.933689</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>gazoza</td>\n",
       "      <td>11.911382</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>adult-cola</td>\n",
       "      <td>8.958909</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>orange-power</td>\n",
       "      <td>25.605094</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>lemon-boost</td>\n",
       "      <td>28.273691</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>kinder-cola</td>\n",
       "      <td>25.085382</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gazoza</td>\n",
       "      <td>25.986615</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>adult-cola</td>\n",
       "      <td>31.680018</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gazoza</td>\n",
       "      <td>49.675375</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>orange-power</td>\n",
       "      <td>33.550887</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lemon-boost</td>\n",
       "      <td>27.776305</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kinder-cola</td>\n",
       "      <td>25.655448</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adult-cola</td>\n",
       "      <td>24.262567</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Brand       MAPE  Fold\n",
       "24  orange-power   5.804694   5.0\n",
       "23   lemon-boost   6.218594   5.0\n",
       "22   kinder-cola   5.252834   5.0\n",
       "21        gazoza  10.742014   5.0\n",
       "20    adult-cola   5.304644   5.0\n",
       "19  orange-power   7.972170   4.0\n",
       "18   lemon-boost   6.218352   4.0\n",
       "17   kinder-cola   5.481717   4.0\n",
       "16        gazoza  13.349993   4.0\n",
       "15    adult-cola  11.457783   4.0\n",
       "13   lemon-boost  11.343404   3.0\n",
       "14  orange-power  10.647084   3.0\n",
       "12   kinder-cola   9.933689   3.0\n",
       "11        gazoza  11.911382   3.0\n",
       "10    adult-cola   8.958909   3.0\n",
       "9   orange-power  25.605094   2.0\n",
       "8    lemon-boost  28.273691   2.0\n",
       "7    kinder-cola  25.085382   2.0\n",
       "6         gazoza  25.986615   2.0\n",
       "5     adult-cola  31.680018   2.0\n",
       "1         gazoza  49.675375   1.0\n",
       "4   orange-power  33.550887   1.0\n",
       "3    lemon-boost  27.776305   1.0\n",
       "2    kinder-cola  25.655448   1.0\n",
       "0     adult-cola  24.262567   1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Result of validation set on each fold\n",
    "final_result.sort_values(by=\"Fold\",ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50ea6f6-5b5d-4e80-9b17-c2a2fae4e9b8",
   "metadata": {},
   "source": [
    "## LSTM using darts\n",
    "\n",
    "### Note on LSTM Model Performance and Nested CV\n",
    "\n",
    "LSTM models typically require large training datasets for optimal performance. However, in our case, the use of nested cross-validation results in smaller training dataset sizes. As a consequence, the model performance may be affected negatively.\n",
    "\n",
    "It's worth noting that for higher values of K (folds), LSTM may become impractical, as the minimum training data size required by LSTM is 25. Given our limited training data size, adjustments to the cross-validation strategy may be necessary to accommodate the specific requirements of LSTM models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc9dae1b-e35c-406c-aa7c-547668c43c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ignoring user defined `output_chunk_length`. RNNModel uses a fixed `output_chunk_length=1`.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 2.8 K \n",
      "4 | V             | Linear           | 26    \n",
      "---------------------------------------------------\n",
      "2.8 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.8 K     Total params\n",
      "0.011     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "831ce8544f5e4c93a2a4eed21459055c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                      | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fab27fcacd647f5ad1c9d482050199e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                    | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mape of adult-cola on validation set is : 23.214739468744387\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d001ee3b6524f37b2f3bbe050c4d449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                    | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mape of gazoza on validation set is : 29.707903528194564\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd8bef13409841948b9260f8d90a38f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                    | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mape of kinder-cola on validation set is : 77.13795942010297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44e24147536840899f00f726041e5f07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                    | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mape of lemon-boost on validation set is : 30.557823120004702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36d5717473734f3a9f8bb87703f889ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                    | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mape of orange-power on validation set is : 23.66890264552309\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Given our limited training data size, increasing the number of folds beyond 1 in the cross-validation process\n",
    "will further reduce the training dataset. This reduction may lead to an error with the LSTM model, as it requires a\n",
    "minimum training data size (25).\"\"\"\n",
    "\n",
    "#Reimporting the mape as the mape variable is used above\n",
    "from darts.metrics import mape\n",
    "cv = NestedCV(1)\n",
    "splits = cv.split(resampled_data, \"date\")\n",
    "for train, validate in splits:\n",
    "    train_series_dict={}\n",
    "    train.set_index(\"date\",inplace=True)\n",
    "    validate.set_index(\"date\",inplace=True)\n",
    "    for brand, group in train.groupby('brand'): \n",
    "        group['quantity'] = group['quantity'].fillna(method='ffill')\n",
    "        ts= TimeSeries.from_dataframe(group[['quantity']], freq='M') \n",
    "        scaler_obj= Scaler() \n",
    "        ts=scaler_obj.fit_transform(ts)\n",
    "        train_series_dict[brand]=ts\n",
    "    all_series = list(train_series_dict.values())\n",
    "    model = RNNModel(model='LSTM', input_chunk_length=4, output_chunk_length=1, n_epochs=5) \n",
    "    # model = NBEATSModel(input_chunk_length=3, output_chunk_length=1, n_epochs=5)\n",
    "    # model = XGBModel(lags=10) \n",
    "    model.fit(all_series)\n",
    "    for brand, group in validate.groupby('brand'):\n",
    "        group['quantity'] = group['quantity'].fillna(method='ffill') \n",
    "        ts= TimeSeries.from_dataframe(group[['quantity']], freq='M') \n",
    "        forecast = model.predict(n=len(ts),series=train_series_dict[brand]) \n",
    "        # Evaluate performance using MAPE \n",
    "        forecast= scaler_obj.inverse_transform(forecast) \n",
    "        mape_value  = mape(ts, forecast)\n",
    "        print(f\"Mape of {brand} on validation set is : {mape_value}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39d4cbb-51ed-4641-9797-d30737840b2d",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "We achieved the highest accuracy with Prophet, as deep learning models demand a larger number of data points for reliable predictions. \n",
    "Additionally, we applied NestedCV to the examples above. Furthermore, there is an opportunity to fine-tune the Prophet model to enhance the accuracy of our predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
